{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***FEATURE ENGINEERING ASSIGNMENT QUES/ANSWERS***"
      ],
      "metadata": {
        "id": "VY6jg9trKvZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a parameter?\n",
        "\n",
        "Ans > A parameter is an internal variable of a machine learning model that is learned from the training data.\n",
        "It defines how the model makes predictions and is adjusted during the training process to minimize errors.\n",
        "\n",
        "Examples:\n",
        "\n",
        "In Linear Regression, parameters are the slope (m) and intercept (b).\n",
        "\n",
        "In Neural Networks, parameters are the weights and biases.\n",
        "\n",
        "Difference from Hyperparameters:\n",
        "\n",
        "Parameters → Learned from data during training (e.g., weights).\n",
        "\n",
        "Hyperparameters → Set before training and control the learning process (e.g., learning rate, number of layers).\n",
        "\n",
        "In short: Parameters are the “settings” that the model learns to make accurate predictions.\n"
      ],
      "metadata": {
        "id": "ZLAcl3FzKvr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is correlation?\n",
        "\n",
        "Ans > Correlation is a statistical measure that shows the strength and direction of the relationship between two variables.\n",
        "\n",
        "Range: Correlation values lie between -1 and 1.\n",
        "\n",
        "+1 → Perfect positive correlation (both variables increase together).\n",
        "\n",
        "-1 → Perfect negative correlation (one increases while the other decreases).\n",
        "\n",
        "0 → No linear relationship between the variables.\n",
        "\n",
        "Example:\n",
        "\n",
        "Height and weight often have a positive correlation — taller people tend to weigh more.\n",
        "\n",
        "The number of hours spent studying and exam scores may also show a positive correlation.\n",
        "\n",
        "Types of correlation:\n",
        "\n",
        "Positive correlation – Both variables move in the same direction.\n",
        "\n",
        "Negative correlation – Variables move in opposite directions.\n",
        "\n",
        "Zero correlation – No relationship.\n",
        "\n",
        "Important Note:\n",
        "Correlation shows association, not cause-and-effect. Two variables can be correlated without one causing the other."
      ],
      "metadata": {
        "id": "F_oybldBKvvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What does negative correlation mean?\n",
        "\n",
        "Ans> Negative correlation means that when one variable increases, the other tends to decrease, and vice versa.\n",
        "It represents an inverse relationship between two variables.\n",
        "\n",
        "Value range: Negative correlation values lie between 0 and -1.\n",
        "\n",
        "-1 → Perfect negative correlation (exact opposite movement).\n",
        "\n",
        "Closer to 0 → Weak negative relationship.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Car fuel in tank vs. distance driven → As distance increases, fuel decreases.\n",
        "\n",
        "Outdoor temperature vs. heater usage → As temperature rises, heater usage falls.\n",
        "\n",
        "Key takeaway:\n",
        "Negative correlation does not necessarily mean that one variable causes the other to decrease — it only shows that they move in opposite directions."
      ],
      "metadata": {
        "id": "AY3Ayg4aKvzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "ANS > Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
        "Instead of following fixed rules, the model improves automatically through experience.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "Dataset\n",
        "\n",
        "The collection of data used for training and testing the model.\n",
        "\n",
        "Usually split into training set and test set (sometimes also a validation set).\n",
        "\n",
        "Model\n",
        "\n",
        "The mathematical or statistical representation that maps input data to output predictions.\n",
        "\n",
        "Examples: Linear Regression, Decision Trees, Neural Networks.\n",
        "\n",
        "Loss Function (Cost Function)\n",
        "\n",
        "A mathematical function that measures how wrong the model’s predictions are compared to actual results.\n",
        "\n",
        "Examples: Mean Squared Error (MSE), Cross-Entropy Loss.\n",
        "\n",
        "Optimization Algorithm\n",
        "\n",
        "The method used to adjust model parameters to minimize the loss function.\n",
        "\n",
        "Examples: Gradient Descent, Adam, RMSProp.\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "Metrics to measure model performance after training.\n",
        "\n",
        "Examples: Accuracy, Precision, Recall, F1-score, R² score.\n",
        "\n",
        "Example:\n",
        "In predicting house prices:\n",
        "\n",
        "Dataset → Historical data with features like area, location, rooms.\n",
        "\n",
        "Model → Linear Regression equation.\n",
        "\n",
        "Loss Function → Mean Squared Error (to measure prediction error).\n",
        "\n",
        "Optimizer → Gradient Descent (to adjust slope & intercept).\n",
        "\n",
        "Metric → R² score to evaluate how well the model explains price variation."
      ],
      "metadata": {
        "id": "YPbRt-O0Kv3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans > The loss value (also called cost or error) is a numerical measure of how far the model’s predictions are from the actual target values.\n",
        "It is calculated using a loss function during training.\n",
        "\n",
        "Why it’s important\n",
        "Lower loss value → Model predictions are closer to actual outcomes → Better performance.\n",
        "\n",
        "Higher loss value → Predictions are far from actual outcomes → Poor performance.\n",
        "\n",
        "It guides the optimizer in adjusting parameters to improve accuracy.\n",
        "\n",
        "Example\n",
        "If you’re predicting house prices:\n",
        "\n",
        "Actual price: ₹50,00,000\n",
        "\n",
        "Model predicts: ₹48,00,000\n",
        "\n",
        "Loss function (e.g., Mean Squared Error) calculates the error between the two.\n",
        "\n",
        "During training, the optimizer adjusts model parameters to reduce this error.\n",
        "\n",
        "Important Notes\n",
        "A low training loss but high test loss means the model is overfitting (memorizing training data instead of generalizing).\n",
        "\n",
        "Comparing loss over epochs helps track learning progress and decide when to stop training.\n",
        "\n",
        "In short: Loss value is the model’s “report card” showing how well it’s learning.\n"
      ],
      "metadata": {
        "id": "N0pJdKJDKv6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What are continuous and categorical variables?\n",
        "\n",
        "In Machine Learning, variables (or features) are the attributes used to describe data. They can be broadly classified into continuous and categorical.\n",
        "\n",
        "1. Continuous Variables\n",
        "Definition: Variables that can take any numeric value within a given range.\n",
        "\n",
        "Nature: Measured, not counted.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 170.5 cm)\n",
        "\n",
        "Temperature (e.g., 36.8°C)\n",
        "\n",
        "Weight (e.g., 65.25 kg)\n",
        "\n",
        "Key Property: Infinite possible values (within limits).\n",
        "\n",
        "2. Categorical Variables\n",
        "Definition: Variables that represent categories or groups, not actual numeric measurements.\n",
        "\n",
        "Nature: Counted, not measured.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender (Male, Female, Other)\n",
        "\n",
        "City (Mumbai, Delhi, Chennai)\n",
        "\n",
        "Blood Group (A, B, AB, O)\n",
        "\n",
        "Key Property: Finite set of possible values.\n",
        "\n",
        "Subtypes of Categorical Variables\n",
        "Nominal: Categories without any order (e.g., colors – red, blue, green).\n",
        "\n",
        "Ordinal: Categories with a meaningful order (e.g., small, medium, large).\n",
        "\n",
        "In short:\n",
        "\n",
        "Continuous → Numeric, infinite possibilities (measurements).\n",
        "\n",
        "Categorical → Labels or groups (classifications)."
      ],
      "metadata": {
        "id": "qhWDHXCaKv9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Ans > Many Machine Learning models work only with numerical inputs, so categorical variables must be encoded into numbers before training.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables\n",
        "1. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "Color: Red → 0, Blue → 1, Green → 2\n",
        "Pros: Simple, memory efficient.\n",
        "\n",
        "Cons: Can imply an order that doesn’t exist (problem for nominal variables).\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates a separate binary column for each category (1 for presence, 0 for absence).\n",
        "\n",
        "Example:\n",
        "\n",
        "less\n",
        "Copy\n",
        "Edit\n",
        "Color: Red → [1, 0, 0]\n",
        "Color: Blue → [0, 1, 0]\n",
        "Color: Green → [0, 0, 1]\n",
        "Pros: Avoids false ordering.\n",
        "\n",
        "Cons: Increases dimensionality (especially with many categories).\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Assigns integers according to the natural order of categories.\n",
        "\n",
        "Example:\n",
        "\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "Size: Small → 1, Medium → 2, Large → 3\n",
        "Pros: Preserves meaningful order.\n",
        "\n",
        "Cons: Not suitable for unordered categories.\n",
        "\n",
        "4. Frequency Encoding\n",
        "Replaces each category with its frequency in the dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "bash\n",
        "Copy\n",
        "Edit\n",
        "City: Delhi (100 times) → 100\n",
        "       Mumbai (80 times) → 80\n",
        "Pros: Useful for high-cardinality features.\n",
        "\n",
        "Cons: Can introduce bias if frequency correlates with target.\n",
        "\n",
        "5. Target Encoding\n",
        "Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "Example (for classification): If “Male” has 0.7 average target value and “Female” has 0.5, encode as such.\n",
        "\n",
        "Pros: Can capture relationship between category and target.\n",
        "\n",
        "Cons: Risk of data leakage if not done carefully.\n",
        "\n",
        "Tip:\n",
        "\n",
        "For tree-based models (e.g., Random Forest, XGBoost), label encoding is often fine.\n",
        "\n",
        "For linear models or distance-based algorithms, one-hot encoding is usually better.\n",
        "\n"
      ],
      "metadata": {
        "id": "qABJ2SgnKwDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is a Test set?\n",
        "\n",
        "Ans > A Test set is a portion of the dataset that is kept separate from the training process and used only to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "Purpose of a Test Set\n",
        "To check how well the model generalizes to unseen data.\n",
        "\n",
        "To provide an unbiased evaluation of the final model’s performance.\n",
        "\n",
        "To detect overfitting — when a model performs well on training data but poorly on new data.\n",
        "\n",
        "Key Points\n",
        "The test set must not be used during model training or parameter tuning.\n",
        "\n",
        "Common split ratios: 70% training / 30% testing or 80% training / 20% testing.\n",
        "\n",
        "Metrics calculated on the test set give the true performance estimate.\n",
        "\n",
        "Example\n",
        "If you have 1,000 rows of data:\n",
        "\n",
        "800 rows → Training set (model learns patterns here).\n",
        "\n",
        "200 rows → Test set (model is evaluated here)."
      ],
      "metadata": {
        "id": "JWa6bMw_MQRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.How do you approach a Machine Learning problem?\n",
        "\n",
        "Ans > Approaching a Machine Learning problem requires a structured, step-by-step process to ensure efficiency and accuracy.\n",
        "\n",
        "Step-by-Step Approach\n",
        "1. Define the Problem\n",
        "Clearly understand what you want to predict or classify.\n",
        "\n",
        "Identify whether it’s a classification, regression, or clustering problem.\n",
        "\n",
        "Example: Predicting house prices → Regression.\n",
        "\n",
        "2. Gather Data\n",
        "Collect relevant data from databases, APIs, CSV files, or sensors.\n",
        "\n",
        "Ensure the data is representative of the real-world scenario.\n",
        "\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "Understand data distribution, detect missing values, and identify outliers.\n",
        "\n",
        "Use visualizations (e.g., histograms, scatter plots, heatmaps) to find patterns.\n",
        "\n",
        "4. Data Preprocessing\n",
        "Handle missing values, remove duplicates, and correct errors.\n",
        "\n",
        "Encode categorical variables, scale numerical values if needed.\n",
        "\n",
        "5. Feature Engineering\n",
        "Create new features, select important ones, and remove irrelevant features.\n",
        "\n",
        "Helps improve model performance.\n",
        "\n",
        "6. Choose a Suitable Model\n",
        "Based on problem type and data size, select an algorithm (e.g., Linear Regression, Decision Tree, Neural Network).\n",
        "\n",
        "7. Train the Model\n",
        "Feed the training set into the chosen model using model.fit().\n",
        "\n",
        "8. Evaluate the Model\n",
        "Use the test set to measure performance using metrics like accuracy, precision, recall, RMSE, etc.\n",
        "\n",
        "9. Hyperparameter Tuning\n",
        "Adjust model hyperparameters using Grid Search, Random Search, or Bayesian Optimization.\n",
        "\n",
        "10. Deploy the Model\n",
        "Integrate the model into a real-world application for predictions.\n",
        "\n",
        "11. Monitor & Maintain\n",
        "Track model performance over time and retrain if accuracy drops due to changing data trends.\n",
        "\n",
        "Example Workflow:\n",
        "Predicting loan defaults → Collect customer data → EDA → Preprocess → Select model (Logistic Regression) → Train → Evaluate → Tune → Deploy → Monitor."
      ],
      "metadata": {
        "id": "5BjYDqqCMQVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans > EDA (Exploratory Data Analysis) is the process of examining and visualizing data before building a model.\n",
        "It helps you understand the dataset’s structure, spot issues, and make better modeling decisions.\n",
        "\n",
        "Reasons to Perform EDA Before Modeling\n",
        "1. Understand the Data\n",
        "Identify data types (numeric, categorical, text, etc.).\n",
        "\n",
        "Check for missing values, duplicates, or outliers.\n",
        "\n",
        "2. Detect Patterns & Relationships\n",
        "Visualize trends and relationships between features and target variables.\n",
        "\n",
        "Example: Higher education levels may correlate with higher salaries.\n",
        "\n",
        "3. Identify Data Quality Issues\n",
        "Spot incorrect entries, format inconsistencies, or anomalies.\n",
        "\n",
        "Prevents feeding bad data into the model.\n",
        "\n",
        "4. Guide Feature Engineering\n",
        "Decide which features to keep, transform, or combine.\n",
        "\n",
        "Example: Combining day, month, and year into a date column.\n",
        "\n",
        "5. Choose the Right Algorithms\n",
        "Some models require scaled data (e.g., Logistic Regression), while others can handle raw data (e.g., Decision Trees).\n",
        "\n",
        "EDA helps determine preprocessing needs.\n",
        "\n",
        "6. Prevent Wasted Effort\n",
        "Saves time by catching problems early instead of after training.\n",
        "\n",
        "Example\n",
        "If you skip EDA:\n",
        "\n",
        "Your dataset might have missing target values or categorical strings that a numerical model can’t handle.\n",
        "\n",
        "Model accuracy drops, and debugging becomes harder.\n",
        "\n",
        "In short:\n",
        "EDA is like a health check-up for your data — it ensures your dataset is clean, relevant, and ready before you train the model."
      ],
      "metadata": {
        "id": "s8rkeW7DMQZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans > An optimizer in Machine Learning is an algorithm that updates the model’s parameters (weights and biases) during training to minimize the loss function and improve accuracy.\n",
        "It decides how and by how much the parameters change after each iteration.\n",
        "\n",
        "Why Optimizers are Important\n",
        "Without optimizers, the model won’t improve because parameters won’t be adjusted efficiently.\n",
        "\n",
        "They control the speed (learning rate) and direction of learning.\n",
        "\n",
        "Types of Optimizers\n",
        "1. Gradient Descent (GD)\n",
        "Calculates the gradient (slope) of the loss function and moves in the opposite direction to minimize loss.\n",
        "\n",
        "Types:\n",
        "\n",
        "Batch GD – Uses the entire dataset for each update.\n",
        "\n",
        "Stochastic GD (SGD) – Uses one sample per update (faster but noisier).\n",
        "\n",
        "Mini-batch GD – Uses small batches for efficiency.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "2. Momentum\n",
        "Improves SGD by remembering the previous update direction to speed up convergence.\n",
        "\n",
        "Works like a rolling ball that builds speed downhill.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "SGD(learning_rate=0.01, momentum=0.9)\n",
        "3. RMSProp (Root Mean Square Propagation)\n",
        "Adapts the learning rate for each parameter based on the moving average of recent gradients.\n",
        "\n",
        "Works well for recurrent neural networks (RNNs).\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "4. Adam (Adaptive Moment Estimation)\n",
        "Combines Momentum and RMSProp for faster and more stable convergence.\n",
        "\n",
        "One of the most widely used optimizers in deep learning.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "5. Adagrad\n",
        "Adjusts the learning rate for each parameter individually based on how frequently it’s updated.\n",
        "\n",
        "Works well for sparse data (e.g., text processing).\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "Tip:\n",
        "\n",
        "Adam is a good default choice for most problems.\n",
        "\n",
        "For very large datasets, SGD with Momentum can be more efficient."
      ],
      "metadata": {
        "id": "q4SmZyWtMQce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model?\n",
        "\n",
        "sklearn.linear_model is a module in Scikit-learn that provides linear models for regression and classification tasks.\n",
        "These models work by assuming a linear relationship between input features (X) and the target variable (y).\n",
        "\n",
        "Main Functions of sklearn.linear_model\n",
        "Build and train models for:\n",
        "\n",
        "Regression (predicting continuous values)\n",
        "\n",
        "Classification (predicting discrete categories)\n",
        "\n",
        "Support regularization to prevent overfitting.\n",
        "\n",
        "Common Classes Inside sklearn.linear_model\n",
        "Class\tPurpose\tExample Use Case\n",
        "LinearRegression\tBasic linear regression without regularization\tPredicting house prices\n",
        "LogisticRegression\tClassification model using logistic function\tSpam email detection\n",
        "Ridge\tLinear regression with L2 regularization\tPredicting sales while controlling overfitting\n",
        "Lasso\tLinear regression with L1 regularization\tFeature selection in datasets\n",
        "ElasticNet\tCombination of L1 and L2 regularization\tPredicting medical risks\n",
        "SGDRegressor / SGDClassifier\tLinear models trained with Stochastic Gradient Descent\tLarge-scale ML problems\n",
        "\n",
        "Example – Linear Regression\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "In short:\n",
        "sklearn.linear_model is the go-to module in Scikit-learn for building and training linear models, both for prediction and classification, with or without regularization."
      ],
      "metadata": {
        "id": "eO948suSMQfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans > The model.predict() method in Machine Learning is used to generate predictions from a trained model.\n",
        "It takes input features and outputs the predicted values based on what the model learned during training.\n",
        "\n",
        "Function Purpose\n",
        "Converts learned patterns (parameters/weights) into predicted outputs.\n",
        "\n",
        "Can be used for:\n",
        "\n",
        "Regression → Predict continuous values.\n",
        "\n",
        "Classification → Predict class labels or probabilities (depending on method used).\n",
        "\n",
        "Arguments\n",
        "X → The input data for which predictions are required.\n",
        "\n",
        "Must have the same number of features as the data used in training.\n",
        "\n",
        "Can be:\n",
        "\n",
        "A single sample (reshaped into correct format).\n",
        "\n",
        "Multiple samples in a 2D array/DataFrame.\n",
        "\n",
        "No labels (y) are provided, since predictions are for unknown outcomes.\n",
        "\n",
        "Example – Regression\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict new values\n",
        "y_pred = model.predict(X_test)\n",
        "Example – Classification\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train model\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Predict probabilities\n",
        "probabilities = clf.predict_proba(X_test)\n",
        "In short:\n",
        "model.predict() is how you ask the trained model to guess outputs for new data, and the only required argument is the feature set (X) in the correct shape."
      ],
      "metadata": {
        "id": "Y2EuPq5fMQis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is the process of transforming numerical features so that they are on a similar scale (range or distribution).\n",
        "It’s a data preprocessing step that ensures no feature dominates others just because of its larger numerical range.\n",
        "\n",
        "Why It’s Needed\n",
        "Many algorithms (e.g., KNN, SVM, Logistic Regression, Neural Networks) use distance or gradient-based optimization, which can be distorted if features are on different scales.\n",
        "\n",
        "Prevents bias towards large-value features.\n",
        "\n",
        "Improves training stability and convergence speed.\n",
        "\n",
        "Example\n",
        "If you have two features:\n",
        "\n",
        "Age → 18 to 70\n",
        "\n",
        "Salary → ₹20,000 to ₹2,00,000\n",
        "\n",
        "Without scaling, the model might treat salary as more important simply because its numbers are bigger.\n",
        "\n",
        "Common Feature Scaling Methods\n",
        "Standardization (Z-score Normalization)\n",
        "\n",
        "Transforms data so mean = 0, standard deviation = 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "′\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "X\n",
        "′\n",
        " =\n",
        "σ\n",
        "X−μ\n",
        "​\n",
        "\n",
        "Example: StandardScaler in scikit-learn.\n",
        "\n",
        "Min-Max Normalization\n",
        "\n",
        "Scales features to a given range, usually [0, 1].\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "′\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "𝑋\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "𝑋\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "−\n",
        "𝑋\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "X\n",
        "′\n",
        " =\n",
        "X\n",
        "max\n",
        "​\n",
        " −X\n",
        "min\n",
        "​\n",
        "\n",
        "X−X\n",
        "min\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Example: MinMaxScaler.\n",
        "\n",
        "Robust Scaling\n",
        "\n",
        "Uses median and IQR (interquartile range) → less sensitive to outliers.\n",
        "\n",
        "Example: RobustScaler.\n",
        "\n",
        "Example in Python\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        " In short:\n",
        "Feature scaling levels the playing field for all numerical features so the model can learn fairly and efficiently."
      ],
      "metadata": {
        "id": "zUZgo16hMQlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Explain data encoding\n",
        "\n",
        "Ans > Data encoding is the process of transforming categorical (non-numeric) data into a numerical format so that machine learning models can process it.\n",
        "Most ML algorithms can only work with numbers, so encoding is a crucial step in data preprocessing.\n",
        "\n",
        "Why Data Encoding is Important\n",
        "ML models (except some tree-based ones) cannot directly handle string labels.\n",
        "\n",
        "Encoding ensures that models can interpret categories mathematically without misinterpreting them as having unintended order or scale.\n",
        "\n",
        "Common Data Encoding Techniques\n",
        "1. Label Encoding\n",
        "Assigns an integer to each unique category.\n",
        "\n",
        "Example:\n",
        "\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "Color: Red → 0, Blue → 1, Green → 2\n",
        "Pros: Simple, memory-efficient.\n",
        "\n",
        "Cons: Can imply an order that does not exist (bad for nominal data).\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates separate binary columns for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "less\n",
        "Copy\n",
        "Edit\n",
        "Color: Red → [1, 0, 0]\n",
        "Color: Blue → [0, 1, 0]\n",
        "Color: Green → [0, 0, 1]\n",
        "Pros: Avoids false ordering.\n",
        "\n",
        "Cons: Increases dimensionality when categories are many.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Assigns integers based on the natural order of categories.\n",
        "\n",
        "Example:\n",
        "\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "Size: Small → 1, Medium → 2, Large → 3\n",
        "Pros: Preserves ranking.\n",
        "\n",
        "Cons: Not suitable for unordered categories.\n",
        "\n",
        "4. Frequency Encoding\n",
        "Replaces categories with their occurrence frequency in the dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "bash\n",
        "Copy\n",
        "Edit\n",
        "City: Delhi (100 times) → 100  \n",
        "      Mumbai (80 times) → 80\n",
        "5. Target Encoding\n",
        "Replaces categories with the mean of the target variable for that category.\n",
        "\n",
        "Useful for high-cardinality data.\n",
        "\n",
        "Risk of data leakage if not done carefully.\n",
        "\n",
        "Example in Python\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "encoder = OneHotEncoder()\n",
        "encoded = encoder.fit_transform(data[['Color']]).toarray()"
      ],
      "metadata": {
        "id": "ja5S5JUlMQob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?\n",
        "\n",
        "Ans > sklearn.preprocessing is a module in Scikit-learn that provides a set of data preprocessing tools to prepare raw data for machine learning models.\n",
        "It includes functions for scaling, normalization, encoding, and transformation of features.\n",
        "\n",
        "Why it’s Important\n",
        "Many ML algorithms require data in a specific format or scale.\n",
        "\n",
        "Preprocessing ensures that all features are treated fairly, improving accuracy and training speed.\n",
        "\n",
        "Key Functionalities of sklearn.preprocessing\n",
        "1. Feature Scaling & Normalization\n",
        "StandardScaler → Standardization (mean = 0, std = 1)\n",
        "\n",
        "MinMaxScaler → Scales features to a given range (default [0, 1])\n",
        "\n",
        "RobustScaler → Uses median and IQR (less sensitive to outliers)\n",
        "\n",
        "Normalizer → Scales feature vectors individually to unit norm.\n",
        "\n",
        "2. Encoding Categorical Variables\n",
        "OneHotEncoder → Converts categories into binary columns.\n",
        "\n",
        "LabelEncoder → Assigns numeric codes to labels.\n",
        "\n",
        "OrdinalEncoder → Assigns integers based on category order.\n",
        "\n",
        "3. Polynomial Features\n",
        "PolynomialFeatures → Generates new features by combining existing ones.\n",
        "Example: From x → generates x², x³, etc.\n",
        "\n",
        "4. Imputation (Handling Missing Values)\n",
        "SimpleImputer → Fills missing values with mean, median, or constant.\n",
        "\n",
        "KNNImputer → Fills missing values using k-nearest neighbors.\n",
        "\n",
        "Example in Python\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Encoding\n",
        "encoder = OneHotEncoder()\n",
        "encoded = encoder.fit_transform(pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})).toarray()"
      ],
      "metadata": {
        "id": "dKtcjwi2MQry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans > In Machine Learning, splitting data into training and testing sets is essential to evaluate how well the model generalizes to unseen data.\n",
        "The training set is used to teach the model, and the test set is used to evaluate its performance.\n",
        "\n",
        "Why We Split Data\n",
        "Prevents overfitting (model memorizing training data instead of learning patterns).\n",
        "\n",
        "Ensures unbiased performance evaluation.\n",
        "\n",
        "How to Split Data in Python\n",
        "Using train_test_split from scikit-learn\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example data\n",
        "X = [[1], [2], [3], [4], [5], [6]]\n",
        "y = [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\", X_train)\n",
        "print(\"Test set:\", X_test)\n",
        "Parameters\n",
        "test_size → Proportion of the dataset for testing (e.g., 0.2 = 20% test data).\n",
        "\n",
        "train_size → Optional, to explicitly set training proportion.\n",
        "\n",
        "random_state → Ensures reproducible results.\n",
        "\n",
        "shuffle → Shuffles data before splitting (default = True).\n",
        "\n",
        "Common Split Ratios\n",
        "70% training / 30% testing (small datasets).\n",
        "\n",
        "80% training / 20% testing (medium datasets).\n",
        "\n",
        "90% training / 10% testing (large datasets)."
      ],
      "metadata": {
        "id": "QH3IcmlOKwHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example data\n",
        "X = [[1], [2], [3], [4], [5], [6]]\n",
        "y = [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\", X_train)\n",
        "print(\"Test set:\", X_test)\n"
      ],
      "metadata": {
        "id": "S1H2fm3DN4xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is sklearn.linear_model?\n",
        "\n",
        "Ans > sklearn.linear_model is a module in Scikit-learn that provides a collection of linear models for solving regression and classification problems.\n",
        "These models assume a linear relationship between input features (X) and the target variable (y).\n",
        "\n",
        "Key Features\n",
        "Supports regression (predicting continuous values) and classification (predicting categories).\n",
        "\n",
        "Includes regularized models to prevent overfitting.\n",
        "\n",
        "Works well for interpretable models and fast training.\n",
        "\n",
        "Popular Classes in sklearn.linear_model\n",
        "Class\tPurpose\tExample Use Case\n",
        "LinearRegression\tFits a straight-line relationship between features and target\tPredicting house prices\n",
        "LogisticRegression\tClassification model using logistic (sigmoid) function\tSpam email detection\n",
        "Ridge\tLinear regression with L2 regularization\tSales prediction with overfitting control\n",
        "Lasso\tLinear regression with L1 regularization\tFeature selection\n",
        "ElasticNet\tCombination of L1 & L2 regularization\tMedical risk prediction\n",
        "SGDRegressor / SGDClassifier\tLinear models trained with stochastic gradient descent\tLarge-scale ML problems\n",
        "\n",
        "Example – Linear Regression\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "y2wTD32wOJDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "GyALoUrmOSuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlifsSO0QQOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}